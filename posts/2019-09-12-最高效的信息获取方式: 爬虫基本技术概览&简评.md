# 最高效的信息获取方式: 爬虫基本技术概览&简评
一直以来，我都苦于找不到稳定的优质信息来源，各种信息源，付费资源，RSS，Newsletter等等尝试了无数，却总是不尽人意。

直到我尝试了爬虫之后才发现这才是获取优质的正确姿势，其它那些由第三方提供且无法定制的信息获取方式注定越来越低效，而爬虫则可以随着自己的水平提升越来越高效，所以长远来看，爬虫是相对最高效的方式。

目前爬虫的相关技术远不只一种，这里我简单概括一下有哪几类，评点一下它们的优缺点，希望能帮助想学爬虫的人更好地入门。

# 浏览器插件 vs 后端服务器
理论上说，使用爬虫最简单的方式就是用[Web Scraper](https://chrome.google.com/webstore/detail/web-scraper/jnhgnonknehpejjnehehllkliplmbmhn)这类浏览器插件，通过鼠标点点点的方式无需写代码也能爬取数据。

感兴趣的可以看看这篇入门文章: [爬取2000篇 Matrix 文章，这是一份可视化踹门教程 - 少数派](https://sspai.com/post/54169)

它的优点是简单方便，适合不会编程也不想学编程的人。缺点则是功能太弱，操作麻烦且无法优化，只能做些”小打小闹”的爬虫，不适合长期使用，更不适合爬取大量数据。

而基于编程语言的后端服务器爬虫，不仅可以在本地运行，还可以上传到多台服务器上分布式运行，并且功能上没有限制，是确定了要学习和使用爬虫的人的不二选择。

# Python vs Node.js
[在GitHub上搜索”scrap”](https://github.com/search?q=scrap)，可以看到排名第一的语言是Python，第二是JavaScript，这就是目前相关资源最多，最适合入门爬虫的两门语言。

Python语言本身比JS更简洁，同时相关的学习资源，代码库，工具网站等等也是最丰富的，虽然Python的性能比JS差，但爬虫的性能影响主要是网络速度和硬盘速度，语言的影响并不大。Python真正的缺点是它只是一门后端语言，而爬虫其实不仅仅是后端，还包括前端的可视化，搜索，排序，过滤等等。

而JS则是前后端通用，后端代码写完之后可以接着写前端，不用费劲去找相关工具还不一定找得到能用的，但反过来也因此相关工具远不如Python，很多时候也只能”自己动手”，对编程水平的要求较高，不适合初学者或者不追求技术的人。

总的来说，如果已经会了Python或JS，那么直接用即可，不用专门学另一门，而如果都不会或者都会，就看你是更想和后端打交道(Python)，把前端部分交给别人或工具处理，还是更想和前端打交道(JS)，甚至前后端一手包办。

# 框架 vs 基础库
Python有[Scrapy](https://github.com/scrapy/scrapy)这个目前最流行的爬虫框架，JS也有[Apify SDK](https://github.com/apifytech/apify-js)提供同样的功能，这些框架把爬虫常用的功能封装好，直接调用API就可以使用。

框架的优点是用的人多，学习资源多，相关工具多，总的来说就是可以借助的东西多，而代价则是限制也多，必须按照框架的API去写代码。

如果你的代码写得比框架差，那么使用框架就是既节省时间，又提高代码质量，而如果你的代码写得更好，那么就是既多花时间，又降低代码质量。

从学习的角度上说，框架把底层基础封装之后更难以理解，而直接用基础库虽然费时费力，但更好理解掌握。个人建议先从基础库"学"起，有需要的话再"用"框架。