# Node.js爬虫后端入门: 爬取豆瓣读书数据
一个完整的爬虫分为前端和后端，而后端又可以分为爬取→保存(→提供前端API)，这里先从最简单的静态页面(豆瓣读书)开始，讲解如何爬取需要的(书籍)数据并保存为表格文件(TSV)。

# 基本构思
在豆瓣上找书，一是通过书单，二是通过标签。书单在多年前改版之后又难看难用，并且随着优质用户的流失，豆瓣上早已无法找到好书单。而标签在多年前改版之后，无法按评分排序，按日期排序直接就少一半的书，综合排序人气占比过高。

这也是为什么我不再使用豆瓣，但我至今没找到好的找书方法，只好自己动手改造一下豆瓣读书。总的来说就是把综合排序的书遍历一遍，保存8分以上的书，然后提供真正的按日期|评分|评分人数进行排序的功能。

# request&cheerio
[request](https://github.com/request/request)用于请求HTML，理论上可以用Node自带的https API，但没必要，而且Python也有个request库，可以方便对比或迁移。

[cheerio](https://cheerio.js.org/)用于解析HTML并用类似Jquery的API提取数据，jsdom类似但用的是原生DOM API，理论上也可以自己写，但费时且没什么价值。

# 一个基本的爬虫实现
先来实现一个最基本的爬虫看看效果:
```
request(encodeURI('https://book.douban.com/tag/编程'), (err, res, body) => {
  let $ = cheerio.load(body)
  let result = $('.info').map((_, elem) => {
    let el = $(elem)
    let date = el.find('.pub').text().match(/\d{4}-\d{1,2}-?\d{0,2}/)
    return {
      name: el.find('h2 a').text().replace(/\s/g, ''),
      date: date? date[0] : '',
      rate: el.find('.rating_nums').text(),
      raters: el.find('.pl').text().match(/\d+/)[0]
    }
  })
  console.table(result.get())
})
```

运行后可得:
```
┌─────────┬────────────────────────────────┬──────────────┬───────┬─────────┐
│ (index) │            name                │     date     │ rate  │ raters  │
├─────────┼────────────────────────────────┼──────────────┼───────┼─────────┤
│    0    │     'Python编程:从入门到实践'      │  '2016-7-1'  │ '9.1' │ '2009'  │
│    1    │        '代码大全（第2版）'         │   '2006-3'   │ '9.3' │ '3701'  │
│    2    │'计算机程序的构造和解释(原书第2版):原书第2版'│   '2004-2'   │ '9.5' │'2205'│
│    3    │ '黑客与画家:硅谷创业之父PaulGraham文集'│   '2011-4'   │ '8.7' │ '16799' │
│    4    │        '流畅的Python'             │ '2017-5-15'  │ '9.4' │  '509'  │
│    5    │    '编码:隐匿在计算机软硬件背后的语言'    │      ''      │ '9.2' │ '2783'│
│    6    │  'Python编程快速上手:让繁琐工作自动化'  │  '2016-7-1'  │ '9.0' │  '552' │
│    7    │       '从Python开始学编程'        │ '2016-11-24' │ '8.3' │  '134'  │
│    8    │          '代码整洁之道'           │  '2010-1-1'  │ '8.6' │ '1487'  │
│    9    │           '算法图解'             │   '2017-3'   │ '8.4' │ '1214'  │
│   10    │      'C程序设计语言:第2版·新版'    │   '2004-1'   │ '9.4' │ '4307'  │
│   11    │      'Java编程思想（第4版）'       │   '2007-6'   │ '9.1' │ '3542'  │
│   12    │     '程序员的自我修养:链接、装载与库'│   '2009-4'   │ '8.8' │ '2221'  │
│   13    │    'C++Primer中文版（第5版）'     │  '2013-9-1'  │ '9.3' │ '1458'  │
│   14    │           '码农翻身'             │  '2018-6-1'  │ '9.0' │  '383'  │
│   15    │       '漫画算法：小灰的算法之旅'    │   '2019-5'   │ '9.1' │  '92'   │
│   16    │         '编程珠玑:第2版'          │  '2008-10'   │ '9.1' │ '2007'  │
│   17    │       '重构:改善既有代码的设计'     │      ''      │ '9.1' │ '1695'  │
│   18    │ 'CPrimerPlus（第6版）中文版:第六版'│  '2016-4-1'  │ '9.3' │  '496'  │
│   19    │         '算法（第4版）'           │ '2012-10-1'  │ '9.4' │ '1050'  │
└─────────┴─────────────────────────────────┴──────────────┴───────┴─────────┘
```

通常在请求阶段需要反爬虫并用代理提高速度，但这里只是简单的HTML网页，所以直接`request`即可，由于有中文所以需要`encodeURI`。

在前端浏览器会自动解析HTML为DOM，而在后端就需要自己把得到的HTML用cheerio加载:`let $ = cheerio.load(body)`，为了提高速度所以进一步加载减少范围:`let el = $(elem)`，这样之后就可以用类似Jquery的API获取数据:
```
el.find('.rating_nums').text(),
el.find('.pl').text().match(/\d+/)[0]
```

不是所有的数据都可以直接得到，所以需要配合regex:
```
el.find('.pub').text().match(/\d{4}-\d{1,2}-?\d{0,2}/)
el.find('h2 a').text().replace(/\s/g, '')
```

# API简化
直接用request和cheerio的话略显啰嗦，所以先简化一下:
```
function findText(el) {
  return sel => el.find(sel).text()
}
function req$(url, callback) {
  request(encodeURI(url), (err, _, body) => {
    if (err) console.log(err)
    else callback(cheerio.load(body))
  })
}
```

# 爬取数据
首先需要确定总共爬多少页，不能一直爬下去:
```
function reqFirst(tag) {
  let result = []
  req$(`https://book.douban.com/tag/${tag}`, $ => {
    let max = $('.paginator').text().match(/\d+(?=\D*$)/)[0]
    if (max > 49) max = 49
    console.log(tag)
    pushResult($, result)
    reqRest(tag, result, 1, max)
  })
}
```

这里`$('.paginator').text().match(/\d+(?=\D*$)/)[0]`先找到分页那一行的字，然后提取最后一个数字为最大页数，由于豆瓣超过50页就没数据了，所以最大页数为49。

然后就是根据豆瓣标签的url，比如`https://book.douban.com/tag/编程?start=20&type=T)`来爬取剩下的页数:
```
function reqRest(tag, result, num, max) {
  console.log(num + '/' + max, new Date().toLocaleTimeString())
  req$(`https://book.douban.com/tag/${tag}?start=${20*num}&type=T`, $ => {
    pushResult($, result)
    if (num < max) reqRest(tag, result, num + 1, max)
    else saveResult(tag, result)
  })
}
```

# 保存数据
首先是提取书籍数据并加入到一个数组里面，这里只提取需要的”评分，名字，日期，评分人数，豆瓣链接":
```
function pushResult($, result) {
  $('.info').each((_, elem) => {
    let t = findText($(elem))
    let rate = t('.rating_nums')
    if (rate < 8) return
    let dateMatch = t('.pub').match(/\d{4}-\d{1,2}-?\d{0,2}/)
    result.push({
      rate,
      name: t('h2 a').trim().replace(/\s+/g, ' ').replace(' : ', ': '),
      date: dateMatch ? dateMatch[0] : '-',
      raters: t('.pl').match(/\d+/)[0],
      href: $(elem).find('h2 a').attr('href')
    })
  })
}
```

然后是把这个数组转换成TSV并保存到电脑里:
```
function saveResult(tag, result) {
  let tsv = result.map(o => Object.values(o).join('\t'))
  fs.writeFile(`tsv/${tag}.tsv`, tsv.join('\n'), 'utf8', err => {
    if (err) console.log(err)
  })
}
```

# 提供后端API并调用
为了方便自己和他人使用，提供一个简洁的API:
```
function crawlTags(tags) {
  tags.split(/\s/)
    .forEach((tag, i) =>
      setTimeout(() => reqFirst(tag), i * 100000))
}
```

这样就可以像这样调用:`crawlTags('工作 职场 沟通 社交')`，这里为了避免被封，每个标签延时了100秒。运行完之后就可以用表格软件或者VSCode插件来查看保存的TSV文件。
